{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torchvision import models\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-16T12:55:04.194010Z","iopub.execute_input":"2022-12-16T12:55:04.194373Z","iopub.status.idle":"2022-12-16T12:55:04.199879Z","shell.execute_reply.started":"2022-12-16T12:55:04.194342Z","shell.execute_reply":"2022-12-16T12:55:04.198911Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass GoogLeNet(nn.Module):\n    def __init__(self, num_classes=1000, aux_logits=True, init_weights=False):\n        super(GoogLeNet, self).__init__()\n        self.aux_logits = aux_logits\n\n        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        if self.aux_logits:\n            self.aux1 = InceptionAux(512, num_classes)\n            self.aux2 = InceptionAux(528, num_classes)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.4)\n        self.fc = nn.Linear(1024, num_classes)\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        # N x 3 x 224 x 224\n        x = self.conv1(x)\n        # N x 64 x 112 x 112\n        x = self.maxpool1(x)\n        # N x 64 x 56 x 56\n        x = self.conv2(x)\n        # N x 64 x 56 x 56\n        x = self.conv3(x)\n        # N x 192 x 56 x 56\n        x = self.maxpool2(x)\n\n        # N x 192 x 28 x 28\n        x = self.inception3a(x)\n        # N x 256 x 28 x 28\n        x = self.inception3b(x)\n        # N x 480 x 28 x 28\n        x = self.maxpool3(x)\n        # N x 480 x 14 x 14\n        x = self.inception4a(x)\n        # N x 512 x 14 x 14\n        if self.training and self.aux_logits:    # eval model lose this layer\n            aux1 = self.aux1(x)\n\n        x = self.inception4b(x)\n        # N x 512 x 14 x 14\n        x = self.inception4c(x)\n        # N x 512 x 14 x 14\n        x = self.inception4d(x)\n        # N x 528 x 14 x 14\n        if self.training and self.aux_logits:    # eval model lose this layer\n            aux2 = self.aux2(x)\n\n        x = self.inception4e(x)\n        # N x 832 x 14 x 14\n        x = self.maxpool4(x)\n        # N x 832 x 7 x 7\n        x = self.inception5a(x)\n        # N x 832 x 7 x 7\n        x = self.inception5b(x)\n        # N x 1024 x 7 x 7\n\n        x = self.avgpool(x)\n        # N x 1024 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 1024\n        x = self.dropout(x)\n        x = self.fc(x)\n        # N x 1000 (num_classes)\n        if self.training and self.aux_logits:   # eval model lose this layer\n            return x, aux2, aux1\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n#inception结构\nclass Inception(nn.Module):\n    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n        super(Inception, self).__init__()\n\n        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)   # 保证输出大小等于输入大小\n        )\n\n        self.branch3 = nn.Sequential(\n            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)   # 保证输出大小等于输入大小\n        )\n\n        self.branch4 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n        )\n\n    def forward(self, x):\n        branch1 = self.branch1(x)\n        branch2 = self.branch2(x)\n        branch3 = self.branch3(x)\n        branch4 = self.branch4(x)\n\n        outputs = [branch1, branch2, branch3, branch4]\n        return torch.cat(outputs, 1)\n\n#辅助分类器\nclass InceptionAux(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3)\n        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)  # output[batch, 128, 4, 4]\n\n        self.fc1 = nn.Linear(2048, 1024)\n        self.fc2 = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n        x = self.averagePool(x)\n        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n        x = self.conv(x)\n        # N x 128 x 4 x 4\n        x = torch.flatten(x, 1)\n        x = F.dropout(x, 0.5, training=self.training)\n        # N x 2048\n        x = F.relu(self.fc1(x), inplace=True)\n        x = F.dropout(x, 0.5, training=self.training)\n        # N x 1024\n        x = self.fc2(x)\n        # N x num_classes\n        return x\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:55:04.201782Z","iopub.execute_input":"2022-12-16T12:55:04.202728Z","iopub.status.idle":"2022-12-16T12:55:04.233621Z","shell.execute_reply.started":"2022-12-16T12:55:04.202638Z","shell.execute_reply":"2022-12-16T12:55:04.232550Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\n\n#建立自己的dataset train_data\nclass MyDataset(Dataset):\n    def __init__(self, image_path: list, image_class: list, transform=None):\n        self.image_path = image_path\n        self.image_class = image_class\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, item):\n        img = Image.open(self.image_path[item]).convert('RGB')\n#         if img.mode != 'RGB':\n#             raise ValueError(\"image: {} isn't RGB mode\".format(self.image_path[item]))\n        label = self.image_class[item]\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:55:04.236471Z","iopub.execute_input":"2022-12-16T12:55:04.237349Z","iopub.status.idle":"2022-12-16T12:55:04.246947Z","shell.execute_reply.started":"2022-12-16T12:55:04.237313Z","shell.execute_reply":"2022-12-16T12:55:04.246014Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.autograd import Variable\n\nepoch_num=20\nbatch_size=32\nlr=0.0003\npath_photo='../input/bitmoji-faces-gender-recognition/BitmojiDataset/trainimages'\npath_label='../input/bitmoji-faces-gender-recognition/train.csv'\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224))\n])\n\n# temp_name = os.listdir(path_photo)\ndf=pd.read_csv(path_label)\nimage_path=[]\nlabel_path = []\nfor dir in os.listdir(path_photo):\n#     print(dir)\n    image_path.append(path_photo+'/'+str(dir))\n    # 得到所有图片label\n    if df[df.image_id == dir].iloc[0, 1]==-1:\n        label_path.append(0)\n    else:\n        label_path.append(df[df.image_id == dir].iloc[0, 1])\n\nimage_path=np.array(image_path)\nlabel_path=np.array(label_path)\n\ntrain_path,val_path,train_label,val_label=train_test_split(image_path,label_path,test_size=0.2,random_state=1)\n# print(len(train_path),len(val_path))\ntrain_data=MyDataset(train_path,train_label,transform)\nval_data=MyDataset(val_path,val_label,transform)\ntrain_loader = DataLoader(train_data, batch_size, shuffle=False)\nval_loader = DataLoader(val_data, batch_size,shuffle=False)\nprint(train_loader)\nprint(val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:55:04.313715Z","iopub.execute_input":"2022-12-16T12:55:04.314133Z","iopub.status.idle":"2022-12-16T12:55:08.484877Z","shell.execute_reply.started":"2022-12-16T12:55:04.314095Z","shell.execute_reply":"2022-12-16T12:55:08.483911Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"<torch.utils.data.dataloader.DataLoader object at 0x7f3a37ffd310>\n<torch.utils.data.dataloader.DataLoader object at 0x7f3a37ffd290>\n","output_type":"stream"}]},{"cell_type":"code","source":"net=GoogLeNet(num_classes=2, aux_logits=True, init_weights=True)\nif torch.cuda.is_available():\n    net=net.cuda()\n# 参数量\nn_p=sum(x.numel() for x in net.parameters())\nprint(n_p)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:55:08.487070Z","iopub.execute_input":"2022-12-16T12:55:08.487666Z","iopub.status.idle":"2022-12-16T12:55:08.819296Z","shell.execute_reply.started":"2022-12-16T12:55:08.487630Z","shell.execute_reply":"2022-12-16T12:55:08.818315Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"10309430\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion=nn.CrossEntropyLoss()\noptimizer=optim.Adam(net.parameters(), lr=lr) ","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:55:08.820774Z","iopub.execute_input":"2022-12-16T12:55:08.821335Z","iopub.status.idle":"2022-12-16T12:55:08.828666Z","shell.execute_reply.started":"2022-12-16T12:55:08.821296Z","shell.execute_reply":"2022-12-16T12:55:08.827618Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# 训练\nimport time\n# iter=0\nloss=0\nacc=0\nn=0\nprint('start training!')\nstart=time.time()\nfor epoch in range(epoch_num):\n    for img,label in train_loader:\n        # 将数据移到GPU上\n        if torch.cuda.is_available():\n            img, label = img.cuda(), label.cuda()\n        # 先将optimizer梯度先置为0\n        optimizer.zero_grad()\n        out,aux_out1,aux_out2=net(img)\n        # 计算loss,图的终点处\n        loss0=criterion(out,label)\n        loss1=criterion(aux_out1,label)\n        loss2=criterion(aux_out2,label)\n        loss=loss0+loss1*0.3+loss2*0.3\n\n#         反向传播\n        loss.backward()\n        # 更新参数\n        optimizer.step()\n        prect =out.argmax(1)\n        num_correct=(prect==label).sum()\n        acc+=num_correct.item()\n        n+=label.shape[0]\n    print('epoch: {}, loss: {:.4} Acc: {:.6f}'.format(epoch, loss.data.item(),acc / n))\n\nstart= time.time()-start\nprint(start)\nprint(\"train over!\")\nif not os.path.exists('./model_save'):\n    os.mkdir('./model_save')\ntorch.save(net.state_dict,os.path.join('./model_save/','epoch_{}_.pth'.format(epoch+1)))\n#     if (epoch%50 == 0) & (epoch != 0):\n#         i = epoch/50\n#         torch.save(model, 'AlexNet%03d.pth'% i)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T12:55:08.830129Z","iopub.execute_input":"2022-12-16T12:55:08.831656Z","iopub.status.idle":"2022-12-16T13:01:31.212362Z","shell.execute_reply.started":"2022-12-16T12:55:08.831621Z","shell.execute_reply":"2022-12-16T13:01:31.211369Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"start training!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0, loss: 0.3297 Acc: 0.665417\nepoch: 1, loss: 0.1418 Acc: 0.770833\nepoch: 2, loss: 0.26 Acc: 0.831806\nepoch: 3, loss: 0.1706 Acc: 0.862396\nepoch: 4, loss: 0.1508 Acc: 0.884750\nepoch: 5, loss: 0.08943 Acc: 0.900556\nepoch: 6, loss: 0.005148 Acc: 0.913690\nepoch: 7, loss: 0.04562 Acc: 0.923438\nepoch: 8, loss: 0.01574 Acc: 0.929028\nepoch: 9, loss: 0.03756 Acc: 0.935583\nepoch: 10, loss: 0.002322 Acc: 0.941023\nepoch: 11, loss: 0.00135 Acc: 0.944688\nepoch: 12, loss: 0.03566 Acc: 0.948814\nepoch: 13, loss: 0.1468 Acc: 0.952232\nepoch: 14, loss: 0.0004225 Acc: 0.954944\nepoch: 15, loss: 0.008075 Acc: 0.957292\nepoch: 16, loss: 8.536e-05 Acc: 0.959632\nepoch: 17, loss: 0.0008303 Acc: 0.961875\nepoch: 18, loss: 7.594e-05 Acc: 0.963882\nepoch: 19, loss: 0.0001236 Acc: 0.965688\n382.27610969543457\ntrain over!\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n# 模型评估\nnet.eval()\neval_loss=0\neval_acc=0\nfor img,label in val_loader:\n    if torch.cuda.is_available():\n        img=img.cuda()\n        label=label.cuda()\n    out=net(img)\n    loss=criterion(out,label)\n    eval_loss+=loss.item()*label.size(0)\n    prect =out.argmax(1)\n    num_correct=(prect==label).sum()\n    eval_acc+=num_correct.item()\nprint('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(val_data)), eval_acc / (len(val_data))))\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-16T13:01:31.214907Z","iopub.execute_input":"2022-12-16T13:01:31.215517Z","iopub.status.idle":"2022-12-16T13:01:35.015406Z","shell.execute_reply.started":"2022-12-16T13:01:31.215479Z","shell.execute_reply":"2022-12-16T13:01:35.014300Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Test Loss: 0.022495, Acc: 0.996667\n","output_type":"stream"}]},{"cell_type":"code","source":"# 测试\nimport torch\nimport pandas as pd\ntestimg_path='../input/bitmoji-faces-gender-recognition/BitmojiDataset/testimages'\nsubmission_path='../input/bitmoji-faces-gender-recognition/sample_submission.csv'\n\ncnt=0\ntest_df=pd.read_csv(submission_path)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224))\n])\n\n# net.eval()\nwith torch.no_grad():\n    parent_list=os.listdir(testimg_path)\n    parent_list.sort()\n#     print(parent_list)\n    for img_name in parent_list:\n        img_path = os.path.join(testimg_path, img_name)\n        img = Image.open(img_path).convert('RGB')\n        img = transform(img)\n        if torch.cuda.is_available():\n            img=img.cuda()\n        img = img.unsqueeze(0)\n        prect = int(net(img).argmax(1))\n        if prect==0:\n            prect=-1\n#         print(prect)\n        test_df['is_male'].iloc[cnt:cnt+1]=str(prect)\n        cnt+=1\n# test_df.iloc[:,1:2]\nprint(test_df)    \ntest_df.to_csv('./sample_submission.csv',index=False,header=True)\nprint('test_over!')","metadata":{"execution":{"iopub.status.busy":"2022-12-16T13:01:35.016850Z","iopub.execute_input":"2022-12-16T13:01:35.017416Z","iopub.status.idle":"2022-12-16T13:01:47.625919Z","shell.execute_reply.started":"2022-12-16T13:01:35.017378Z","shell.execute_reply":"2022-12-16T13:01:47.624916Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_block(indexer, value, name)\n","output_type":"stream"},{"name":"stdout","text":"      image_id is_male\n0     3000.jpg      -1\n1     3001.jpg      -1\n2     3002.jpg      -1\n3     3003.jpg       1\n4     3004.jpg      -1\n...        ...     ...\n1079  4079.jpg      -1\n1080  4080.jpg       1\n1081  4081.jpg       1\n1082  4082.jpg      -1\n1083  4083.jpg       1\n\n[1084 rows x 2 columns]\ntest_over!\n","output_type":"stream"}]}]}